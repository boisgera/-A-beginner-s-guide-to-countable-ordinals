<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Timothy Gowers" />
  <title>A beginner’s guide to countable ordinals</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
  </style>
  <link rel="stylesheet" href="css/style.css" />
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Alegreya:ital,wght@0,400;0,500;0,600;0,700;1,400;1,500;1,600;1,700&display=swap" rel="stylesheet">
  <script>
  document.addEventListener("DOMContentLoaded", (event) => {
      let h3 = document.createElement("h3");
      h3.innerHTML = "Table of contents";
      let toc = document.getElementById("TOC");
      toc.prepend(h3);
  });    
  </script>
</head>
<body>
<header id="title-block-header">
<h1 class="title">A beginner’s guide to countable ordinals</h1>
<p class="author">Timothy Gowers</p>
</header>
<nav id="TOC" role="doc-toc">
<ul>
<li><a href="#prerequisites">Prerequisites</a></li>
<li><a href="#contents">Contents</a></li>
<li><a href="#three-problems">Three problems</a>
<ul>
<li><a href="#problem-1.-showing-that-a-continuous-function-on-01-is-bounded">Problem 1. Showing that a continuous function on <span class="math inline">\([0,1]\)</span> is bounded</a></li>
<li><a href="#problem-2.-generating-the-borel-sets">Problem 2. Generating the Borel sets</a></li>
<li><a href="#problem-3.-showing-that-open-games-are-determined">Problem 3. Showing that open games are determined</a></li>
</ul></li>
<li><a href="#wosets">Well-ordered sets</a>
<ul>
<li><a href="#definition-of-well-ordered-sets">Definition of well-ordered sets</a></li>
<li><a href="#maps-between-well-ordered-sets">Maps between well-ordered sets</a></li>
<li><a href="#order-types-of-well-ordered-sets">Order-types of well-ordered sets</a></li>
</ul></li>
<li><a href="#using">How to use ordinals</a></li>
<li><a href="#uncountablewoset">Proof that there is an uncountable well-ordered set</a></li>
<li><a href="#notusing">How not to use ordinals</a>
<ul>
<li><a href="#induction">Ordinals and induction</a></li>
<li><a href="#solving-problems-1-and-3-without-ordinals.">Solving problems 1 and 3 without ordinals.</a></li>
</ul></li>
<li><a href="#undetermined">A very brief sketch of how to “construct” a game that is not determined</a></li>
<li><a href="#infplusone">How can infinity plus one not be infinity?</a></li>
</ul>
</nav>
<h3 id="prerequisites">Prerequisites</h3>
<p>It would help to know basic real analysis, up to the definition of open and closed sets. But even without this, it might be possible to understand the general discussion and at least one of the examples.</p>
<h3 id="contents">Contents</h3>
<p>When one meets ordinals for the first time in a university course on set theory, they can seem quite forbidding. In particular, the notion of a transitive set (a set <span class="math inline">\(X\)</span> such that every element of <span class="math inline">\(X\)</span> is also a subset of <span class="math inline">\(X\)</span>) takes a little getting used to, and seems rather artificial. In fact, it <em>is</em> rather artificial – one would like to define an ordinal to be an equivalence class of well-ordered sets (if you don’t know what I mean by this, then you can come back to these preliminary remarks after reading the explanations below) but this creates difficulties of a the-set-of-all-sets kind, and is therefore not allowed. Instead one is forced to come up with a way of constructing a well-ordered set of each order-type, and this can be done in various ways.</p>
<p>The aims of this page are to show</p>
<ul>
<li>that it is possible to <em>use</em> ordinals without knowing how they are constructed (rather as one can do real analysis without knowing how real numbers are constructed);</li>
<li>that the countable ordinals, which are sufficient for many applications, can be constructed rigorously in a way that requires more or less no knowledge of set theory;</li>
<li>that ordinals crop up naturally in many contexts, so that it is worth understanding them, at least in the naive way outlined below.</li>
</ul>
<h2 id="three-problems">Three problems</h2>
<p>The idea of this section is to show how ordinals arise naturally in three contexts. Quite a bit of the discussion, particularly with Problems 2 and 3, is about the contexts themselves rather than about ordinals. I have therefore given various opportunities for the reader to jump to the end of the section and start reading the more general theory. The first one is <a href="#wosets">now</a>.</p>
<h3 id="problem-1.-showing-that-a-continuous-function-on-01-is-bounded">Problem 1. Showing that a continuous function on <span class="math inline">\([0,1]\)</span> is bounded</h3>
<p>I have discussed this problem in detail on a <a href="https://www.dpmms.cam.ac.uk/~wtg10/bounded.html">different page</a>. Here I shall be a little briefer about the real analysis but much more detailed about the aspect of the problem that relates to ordinals.</p>
<p>If <span class="math inline">\(f\)</span> is continuous on <span class="math inline">\([0, 1]\)</span> and bounded on <span class="math inline">\([0, t]\)</span>, then, by continuity at <span class="math inline">\(t\)</span>, one can find <span class="math inline">\(s&gt;t\)</span> such that <span class="math inline">\(f\)</span> is bounded on the interval <span class="math inline">\([0, s]\)</span>. Let us call this the <strong>basic lemma</strong> . Using the basic lemma repeatedly, one can build a sequence of numbers <span class="math inline">\(0 &lt; t_1 &lt; t_2 &lt; t_3 &lt; \dots\)</span> in such a way that <span class="math inline">\(f\)</span> is bounded on the interval <span class="math inline">\([0, t_n]\)</span> for every <span class="math inline">\(n\)</span> (though so far we do not know whether <span class="math inline">\(f\)</span> is bounded on the union of all those intervals).</p>
<p>Since the <span class="math inline">\(t_n\)</span> all live in <span class="math inline">\([0, 1]\)</span>, they form a monotonic sequence which is bounded above. Hence, they converge to some limit, which, with the considerable benefit of hindsight, I shall call <span class="math inline">\(t_{\omega}\)</span>. By continuity, there is some interval around <span class="math inline">\(t_{\omega}\)</span> on which <span class="math inline">\(f\)</span> is bounded. Since this interval contains <span class="math inline">\(t_n\)</span> for all sufficiently large <span class="math inline">\(n\)</span>, we can deduce that <span class="math inline">\(f\)</span> is bounded all the way to <span class="math inline">\(t_{\omega}\)</span>. Thus, <span class="math inline">\(f\)</span> <em>was</em> bounded on the union of the intervals <span class="math inline">\([0, t_n]\)</span>, though what we have now proved is very slightly stronger as it includes the end point <span class="math inline">\(t_{\omega}\)</span>.</p>
<p>Now we can continue the sequence “beyond infinity” by using the basic lemma repeatedly, starting at <span class="math inline">\(t_{\omega}\)</span>. This gives us a second sequence <span class="math inline">\(t_{\omega} &lt; t_{\omega+1} &lt; t_{\omega+2} &lt; \dots\)</span> such that <span class="math inline">\(f\)</span> is bounded on the interval <span class="math inline">\([0,t_{\omega+n}]\)</span> for every <span class="math inline">\(n\)</span>.</p>
<p>It may look as though there is something not quite rigorous going on here. If you think of <span class="math inline">\(\omega\)</span> as being infinity (as it is quite reasonable to do) then is it legitimate to start adding <span class="math inline">\(1, 2, 3,\dots\)</span> to it? Doesn’t infinity plus one equal infinity?</p>
<p>I shall return to this question, but for the time being let me say only this: there is nothing to stop me using expressions like <span class="math inline">\(\omega+17\)</span> if all I do is regard them as <em>notation</em> . Instead of writing <span class="math inline">\(t_{\omega+n}\)</span> I could have written <span class="math inline">\(u_n\)</span>. Or I could have written <span class="math inline">\(t_{0n}\)</span> for <span class="math inline">\(t_n\)</span> and then <span class="math inline">\(t_{1n}\)</span> for <span class="math inline">\(t_{\omega+n}\)</span>.</p>
<p>Thus reassured, let us continue. By the same argument as before, the sequence <span class="math inline">\((t_{\omega+n})\)</span> converges, and it feels natural to write <span class="math inline">\(t_{2\omega}\)</span> for its limit. Again, as before, <span class="math inline">\(f\)</span> is bounded on the interval <span class="math inline">\([0,t_{2\omega}]\)</span>.</p>
<p>Now we can continue the sequence – or perhaps one should say generalized sequence – with <span class="math inline">\(t_{2\omega+1}\)</span>, <span class="math inline">\(t_{2\omega+2}\)</span> and so on. These converge to a number we can call <span class="math inline">\(t_{3\omega}\)</span>, and if we keep on going we will end up producing, for every <span class="math inline">\(m&gt; 0\)</span> and <span class="math inline">\(n &gt; 0\)</span>, a number <span class="math inline">\(t_{m\omega+n}\)</span> (I shall interpret <span class="math inline">\(t_0\)</span> as <span class="math inline">\(0\)</span>) such that <span class="math inline">\(f\)</span> is bounded on the interval <span class="math inline">\([0,t_{m\omega+n}]\)</span>. Furthermore, <span class="math inline">\(t_{m\omega+n}\)</span> will be greater than <span class="math inline">\(t_{k\omega+l}\)</span> if and only if <span class="math inline">\(m &gt; k\)</span> or <span class="math inline">\(m=k\)</span> and <span class="math inline">\(n &gt; l\)</span>.</p>
<p>What can we say now? What if none of these numbers is bigger than <span class="math inline">\(1/100\)</span>, as may well be the case? Well, we are not forced to stop, because we could consider the increasing sequence <span class="math inline">\(t_{\omega}, t_{2\omega}, t_{3\omega}, \dots\)</span>. This has a limit, which is easily seen to be bigger than any of the numbers <span class="math inline">\(t_{m\omega+n}\)</span>, and what’s more there is a natural name for this limit – <span class="math inline">\(t_{\omega^2}\)</span>. Once again, <span class="math inline">\(f\)</span> will be bounded on the interval <span class="math inline">\([0,t_{\omega^2}]\)</span>.</p>
<p>For convenience, let me state and prove a <strong>second basic lemma</strong>, the one that we use for limits of sequences. It says that if we have an increasing sequence of numbers <span class="math inline">\(u_1, u_2, \dots\)</span> in <span class="math inline">\([0, 1]\)</span> and if <span class="math inline">\(f\)</span> is a continuous function that is bounded on the interval <span class="math inline">\([0,u_n]\)</span> for every <span class="math inline">\(n\)</span>, then the <span class="math inline">\(u_n\)</span> converge to a limit <span class="math inline">\(u\)</span> and <span class="math inline">\(f\)</span> is bounded on the interval <span class="math inline">\([0,u]\)</span>.</p>
<p>The fact that the <span class="math inline">\(u_n\)</span> converge is just the monotone-sequences axiom for the real numbers. As for the boundedness of <span class="math inline">\(f\)</span> on <span class="math inline">\([0, u]\)</span>, I proved it earlier, but let me repeat the argument. By continuity, <span class="math inline">\(f\)</span> is bounded in some interval about <span class="math inline">\(u\)</span>. This interval contains <span class="math inline">\(u_n\)</span> for all sufficiently large <span class="math inline">\(n\)</span>, and in particular for <em>some</em> <span class="math inline">\(n\)</span>. Hence, putting together the interval <span class="math inline">\([0,u_n]\)</span> and the interval surrounding <span class="math inline">\(u\)</span>, we find that <span class="math inline">\(f\)</span> is bounded on <span class="math inline">\([0, u]\)</span>.</p>
<p>Now that I have gone this far, it should be clear how to continue, obtaining numbers <span class="math inline">\(t_{l\omega^2+m\omega+n}\)</span> for any <span class="math inline">\(l,m,n &gt; 0\)</span>. And, of course, this is still not the end. One can construct <span class="math inline">\(t_{p(\omega)}\)</span> for any polynomial <span class="math inline">\(p\)</span> with positive integer coefficients, in such a way that they come in the obvious order (a neat way to describe it is that <span class="math inline">\(t_{p(\omega)} &gt; t_{q(\omega)}\)</span> if and only if <span class="math inline">\(p(n) &gt; q(n)\)</span> for all sufficiently large integers <span class="math inline">\(n\)</span>) and such that <span class="math inline">\(f\)</span> is bounded on all the intervals <span class="math inline">\([0,t_{p(\omega)}]\)</span>. Then we can apply the second basic lemma to the sequence <span class="math inline">\(t_{\omega}\)</span>, <span class="math inline">\(t_{\omega^2}\)</span>, <span class="math inline">\(t_{\omega^3}\)</span>, <span class="math inline">\(\dots\)</span> obtaining a limit, which is called – as you have probably guessed – <span class="math inline">\(t_{\omega^{\omega}}\)</span>. I now leave it to you, if you are interested, to work out how to continue further, to <span class="math inline">\(t_{\omega^{\omega^{\omega}}}\)</span> and beyond, and then to take a limit <span class="math inline">\(t_{\epsilon_0}\)</span>, where <span class="math inline">\(\epsilon_0\)</span> is the standard notation for what one might otherwise call <span class="math inline">\(\omega\)</span> to the <span class="math inline">\(\omega\)</span> to the <span class="math inline">\(\omega\)</span> to the <span class="math inline">\(\omega\)</span> to the … and so on.</p>
<p>What is the point of all this? There is nothing to force even <span class="math inline">\(t_{\epsilon_0}\)</span> to be close to <span class="math inline">\(1\)</span>, and it begins to look as though, however ingenious we are in extending our notation in order to lengthen the”sequence”, we will never manage to reach <span class="math inline">\(1\)</span> and hence never prove the theorem.</p>
<p>Is it conceivable that we could go <em>so far</em> with the process that we eventually succeeded in forcing the “sequence” to go beyond <span class="math inline">\(1\)</span>? In one way the answer is yes, and in another it is no.</p>
<p>The following observation gives us cause for optimism. If we could explain how to extend the sequence <em>uncountably</em> many times, then we would have proved the theorem. That is because every number in the “sequence” is followed by an open interval that contains no others, and these open intervals are disjoint. Since any open interval contains a rational number, there can be at most countably many of them. This would give us a contradiction, deduced from the assumption that we never passed 1 together with our method, whatever it might be, for continuing the process of extending uncountably many times.</p>
<p>On the other hand, a second observation should give us pause for thought. If you look at the discussion above, you will see that at various points I had to make a “notational leap”. Each of the following symbols required of me a small explanation, after which it became obvious for a while how to proceed: <span class="math inline">\(\omega\)</span>, <span class="math inline">\(\omega+1\)</span>, <span class="math inline">\(2\omega\)</span>, <span class="math inline">\(\omega^2\)</span>, <span class="math inline">\(\omega^{\omega}\)</span>, <span class="math inline">\(\epsilon_0\)</span>. Given that there are only countably many finite strings of symbols, it will not be possible to continue to find an adequate notation for uncountably many terms in the sequence (hypothetical terms that is – we eventually hope for a contradiction). That does not rule out a sort of infinitely long pseudo-notation (such as we use for real numbers – it is somehow comforting to write <span class="math inline">\(\pi = 3.14159265358979323846264338327950288\dots\)</span> and imagine the sequence going on for ever) but it does suggest that we should not be too <em>reliant</em> on notation. There is in fact a theorem of logic (whose precise statement, alas, I do not know) which says something like that one does indeed have to continue making notational leaps, and that eventually it ceases to be possible.</p>
<p>Click <a href="#wosets">here</a> if you would now like to skip to a general discussion of well-ordered sets and ordinals.</p>
<h3 id="problem-2.-generating-the-borel-sets">Problem 2. Generating the Borel sets</h3>
<p>A <em>Borel set</em> of real numbers is a set that can be built out of open or closed sets by repeatedly taking countable unions or countable intersections. To be precise, open sets and closed sets are Borel, and if <span class="math inline">\(B_1\)</span>, <span class="math inline">\(B_2\)</span>, <span class="math inline">\(B_3\)</span>, <span class="math inline">\(\dots\)</span> are Borel, then so is their union and their intersection.</p>
<p>Let me give a non-trivial example of a Borel set (i.e., one that isn’t something like the union of an open set with a closed set). First, here is a subset <span class="math inline">\(A\)</span> of <span class="math inline">\((0,1)\)</span> which has three properties that might at first seem hard to reconcile: it is open and dense, but is nothing like the whole of <span class="math inline">\((0,1)\)</span>. If you think that a dense open subset of <span class="math inline">\((0,1)\)</span> <em>must</em> be the whole of <span class="math inline">\((0,1)\)</span>, then you should consider the union of <span class="math inline">\((0,1/2)\)</span> and <span class="math inline">\((1/2,1)\)</span> and see where your argument breaks down. The point is that, although every point in some dense set is surrounded by a little interval, you can make the width of that interval depend on the point in order to leave other points out of your set.</p>
<p>Once you have spotted that, then you can go much further. Let <span class="math inline">\(q_1\)</span>, <span class="math inline">\(q_2\)</span>, <span class="math inline">\(q_3\)</span>, <span class="math inline">\(\dots\)</span> be an enumeration of the rationals in <span class="math inline">\([0,1]\)</span> and for each <span class="math inline">\(k\)</span> let <span class="math inline">\(I_k\)</span> be an interval of width <span class="math inline">\(r^{-k}\)</span> centered on <span class="math inline">\(q_k\)</span>. Let <span class="math inline">\(A&#39;(r)\)</span> be the union of all the sets <span class="math inline">\(I_k\)</span> and let <span class="math inline">\(A(r)\)</span> be the intersection of <span class="math inline">\(A&#39;(r)\)</span> with <span class="math inline">\((0,1)\)</span>. For <span class="math inline">\(r &gt; 2\)</span> the sum of the widths of the <span class="math inline">\(I_k\)</span> is less than 1, so <span class="math inline">\(A&#39;(r)\)</span> cannot possibly be the whole of <span class="math inline">\((0,1)\)</span>. (For those who know what this means, it has measure less than 1.) The larger <span class="math inline">\(r\)</span> is, the less of <span class="math inline">\((0,1)\)</span> is contained in <span class="math inline">\(A(r)\)</span>.</p>
<p>Now let <span class="math inline">\(A\)</span> be the intersection of all the <span class="math inline">\(A(r)\)</span>, for <span class="math inline">\(r=1,2,3,\dots\)</span> Since all the <span class="math inline">\(A(r)\)</span> were open, <span class="math inline">\(A\)</span> is a Borel set. What is <span class="math inline">\(A\)</span>? You might guess that <span class="math inline">\(A\)</span> was just the set of all rationals in <span class="math inline">\((0,1)\)</span>, but in fact this is quite wrong. (It is an interesting exercise to see why.) Since <span class="math inline">\(A\)</span> contains all rationals but is not the whole of <span class="math inline">\((0,1)\)</span> (in fact, it has measure zero) it cannot possibly be closed. So it is a good example of a countable intersection of open sets which cannot be described any more simply than that.</p>
<p>Returning to more general Borel sets, let us think what a typical Borel set might look like. Let <span class="math inline">\(U_0\)</span> be the class of all open sets, and let <span class="math inline">\(U_1\)</span> consist of all intersections of countably many sets in <span class="math inline">\(U_0\)</span>. We cannot produce more sets by taking countable intersections of sets in <span class="math inline">\(U_1\)</span> (since such sets will themselves belong to <span class="math inline">\(U_1\)</span>) but there is no guarantee that a countable <em>union</em> of sets in <span class="math inline">\(U_1\)</span> will belong to <span class="math inline">\(U_1\)</span>. In fact, examples can be constructed that don’t.</p>
<p>We can therefore define <span class="math inline">\(U_2\)</span> to be the class of all countable unions of sets in <span class="math inline">\(U_1\)</span>, and in general can define <span class="math inline">\(U_n\)</span> to be the set of all countable unions/intersections of sets in <span class="math inline">\(U_{n-1}\)</span> according to whether <span class="math inline">\(n\)</span> is even/odd.</p>
<p>Does the process stop there? Not at all. We can then define <span class="math inline">\(U_{\omega}\)</span> to be the class of all unions of sets <span class="math inline">\(A_1\)</span>, <span class="math inline">\(A_2\)</span>, <span class="math inline">\(\dots\)</span> such that <span class="math inline">\(A_n\)</span> belongs to <span class="math inline">\(U_n\)</span>. This is a countable union of sets we have so far built, and there is no reason for it to belong to any of the classes <span class="math inline">\(U_n\)</span>. (Incidentally, I am using the word “class” just to distinguish subsets of <span class="math inline">\(\mathbb{R}\)</span> from sets of subsets of <span class="math inline">\(\mathbb{R}\)</span> – not because there is any danger of running into Russell’s paradox.)</p>
<p>Now we can let <span class="math inline">\(U_{\omega+1}\)</span> consist of all countable intersections of sets in <span class="math inline">\(U_{\omega}\)</span>, and so on. In fact, just as with Problem 1, one ends up with a hierarchy that goes well beyond infinity, and about which it gradually becomes harder and harder to speak. I leave it to the reader to work out which sets belong to <span class="math inline">\(U_{\omega^{\omega}}\)</span>.</p>
<p>Click <a href="#wosets">here</a> if you would now like to skip to a general discussion of well-ordered sets and ordinals.</p>
<h3 id="problem-3.-showing-that-open-games-are-determined">Problem 3. Showing that open games are determined</h3>
<p>Many two-player games of skill are special cases of the following general one. Write <span class="math inline">\(\mathbb{N}^{\mathbb{N}}\)</span> for the set of all sequences of positive integers, and let <span class="math inline">\(X\)</span> be a subset of <span class="math inline">\(\mathbb{N}^{\mathbb{N}}\)</span>. Two players of the <span class="math inline">\(X\)</span>-game take turns choosing positive integers, building a sequence <span class="math inline">\((n_1,n_2,n_3,\dots)\)</span>. The first player wins if this sequence belongs to <span class="math inline">\(X\)</span> and the second wins if it doesn’t.</p>
<p>A <em>strategy</em> for the first player is a function <span class="math inline">\(f\)</span> that takes sequences <span class="math inline">\((n_1, n_2, \dots, n_k)\)</span> (with <span class="math inline">\(k\)</span> even) to positive integers. To use the strategy <span class="math inline">\(f\)</span>, all the first player does is look at the sequence <span class="math inline">\((n_1, n_2, \dots, n_k)\)</span> so far and choose <span class="math inline">\(f((n_1, n_2, \dots, n_k))\)</span> as the next move. A <em>winning strategy</em> for the first player is a strategy <span class="math inline">\(f\)</span> such that, using it, the first player is guaranteed to win. Similarly one can define strategies and winning strategies for the second player. A game is called <em>determined</em> if one of the two players has a winning strategy.</p>
<p>How, you might wonder, could there possibly be a game that was not determined? If the first player has no winning strategy, does that not mean that the second player is guaranteed to win, and is that not the same as saying that the second player has a winning strategy?</p>
<p>The sketchy argument of the last paragraph doesn’t work for the following reason. What we can deduce from the first player having no winning strategy is that, given any strategy, the second player can come up with another strategy that defeats it. (This strategy can be very simple – something like choose 2 then 8 then 673 then … etc.) But there is no guarantee that the first player will stick to any one strategy.</p>
<p>But surely, you might still think, whatever the first player does is the result of <em>some</em> strategy, even if it isn’t declared in advance. That is true, but it doesn’t help the second player find a strategy. What we are trying to do is start from the premise “For every first-player strategy <span class="math inline">\(f\)</span> there is a second-player strategy <span class="math inline">\(g\)</span> that defeats <span class="math inline">\(f\)</span>” to the conclusion “There is a second-player strategy <span class="math inline">\(g\)</span> that defeats every first-player strategy <span class="math inline">\(f\)</span>”. In other words, we are trying to interchange two quantifiers and obtain a <span class="math inline">\(g\)</span> that is <em>independent</em> of <span class="math inline">\(f\)</span> – which certainly shouldn’t be trivial.</p>
<p>With the axiom of choice one can in fact build a game that is not determined – using a “just do it” proof of a kind explained <a href="https://www.dpmms.cam.ac.uk/~wtg10/justdoit.html">here</a>. The proof depends on well-orderings so I will give it <a href="#undetermined">later on</a>.</p>
<p>An <em>open</em> game is defined to be one where, given any sequence <span class="math inline">\((n_1, n_2, n_3, \dots)\)</span> in <span class="math inline">\(X\)</span> there exists a <span class="math inline">\(k\)</span> such that all sequences that begin <span class="math inline">\((n_1, n_2, \dots, n_k)\)</span> are also in <span class="math inline">\(X\)</span>. In other words, if the first player wins, then there was some point after which they would have won whatever moves they had played. Here are a few examples of open games. Although these games go on for ever, I shall think of them as finishing when they reach a finite sequence all of whose continuations lie in <span class="math inline">\(X\)</span> – after which point they are not very interesting.</p>
<ol type="1">
<li><p>Let <span class="math inline">\(X\)</span> be the set of all sequences with at least 300 occurrences of the number 10. It is of course very easy for player 1 to win this game – one strategy is to choose the number 10 for the first 300 moves. More to the point, if player 1 wins, there must be some point at which the number 10 has appeared 300 times, after which nothing can affect the outcome of the game.</p></li>
<li><p>Let <span class="math inline">\(X\)</span> be the set of all sequences <span class="math inline">\((n_1, n_2, n_3, \dots)\)</span> with at least <span class="math inline">\(n_{10}\)</span> occurrences of the number 10. Again, player 1 has an easy strategy: wait for player 2 to choose <span class="math inline">\(n_{10}\)</span> and then keep choosing 10s until there are enough of them. Notice a difference between this game and the previous one though: whereas in the previous one player 1 could guarantee to win after 300 moves, in this one player 2 can make the game last arbitrarily long simply by choosing <span class="math inline">\(n_{10}\)</span> arbitrarily large.</p></li>
<li><p>Let <span class="math inline">\(X\)</span> be the set of all sequences <span class="math inline">\((n_1, n_2, n_3, \dots)\)</span> with at least <span class="math inline">\(n_{n_{10}}\)</span> occurrences of the number 10. Once again, player 1 has an easy strategy: wait for player 2 to choose <span class="math inline">\(n_{10}\)</span>, then wait for player 2 to choose <span class="math inline">\(n_{n_{10}}\)</span> and then choose that number of 10s. Whereas with the previous time player 1 could say, “After 10 moves of the game I will be able to tell you how long it will take me to win,” for this game no such assurance can be given. The best that can be said is, “After 10 moves I will be able to tell you when I will know how long the game will take.”</p></li>
</ol>
<p>As is becoming clear, there is a hierarchy of complexity appearing. Let us say that a game has complexity <span class="math inline">\(n\)</span> if player 1 can guarantee to win after <span class="math inline">\(n\)</span> moves. We could describe this inductively by saying that the complexity is 0 if player 1 wins no matter what happens and complexity <span class="math inline">\(n\)</span> if after both players have moved once the complexity of the resulting position is <span class="math inline">\(n-1\)</span>. We can then say that a game has complexity <span class="math inline">\(\omega\)</span> if after the first moves the resulting position has complexity <span class="math inline">\(n\)</span> for <em>some</em> <span class="math inline">\(n\)</span>. Next, it has complexity <span class="math inline">\(\omega+n\)</span> if after the first moves the resulting position has complexity <span class="math inline">\(\omega+n-1\)</span>.</p>
<p>Just to illustrate this: the first game above has complexity 300 and the second has complexity <span class="math inline">\(\omega+4\)</span>. To see why the second statement is true, notice that on the fifth pair of moves the second player will be forced to decide on <span class="math inline">\(n_{10}\)</span> after which the length of the game will be known and hence the complexity will be some positive integer.</p>
<p>A game has complexity <span class="math inline">\(2\omega\)</span> if after the first move the resulting position has complexity <span class="math inline">\(\omega + n\)</span> for some <span class="math inline">\(n\)</span>. It has complexity <span class="math inline">\(\omega^2\)</span> if after the first move the game has complexity <span class="math inline">\(a\omega+b\)</span> for some <span class="math inline">\(a, b\)</span>. (When I say “has complexity …” I mean “has complexity no worse than …”.) It has complexity <span class="math inline">\(\omega^{\omega}\)</span> if after the first move the game has complexity some polynomial in <span class="math inline">\(\omega\)</span>.</p>
<p>And so it goes on, just as with the previous examples. What would player 1 be able to guarantee if the game had complexity <span class="math inline">\(\omega^2\)</span>? They would find it rather hard to explain, but might make an attempt like this: “I will be able to tell you how long it will be until I can tell you how long it will be until I can tell you how long it will be until … and so on … I can tell you how long it will be until I win the game, and after the second player’s first move I will be able to tell you how many times I needed to repeat the words ‘I can tell you how long it will be until’.” I leave it to the reader to give an intuitive description of games of complexity <span class="math inline">\(\omega^{\omega}\)</span>.</p>
<p>Before moving on, let us see why every game of complexity <span class="math inline">\(\omega^2\)</span> is determined. I shall assume that every game of complexity <span class="math inline">\(m\omega+n\)</span> is determined (as an inductive hypothesis). If player 2 has no winning strategy, there must be some first move for player 1 such that player 2 still has no winning strategy. After they have both had their first turn, the position is a game of complexity <span class="math inline">\(m\omega+n\)</span> for some <span class="math inline">\(m,n\)</span>, and is therefore determined. Since player 2 does not have a winning strategy, player 1 must. But this gives player 1 a strategy for the whole game – do that first move and then apply whatever winning strategy is appropriate after player 2’s first move.</p>
<h2 id="wosets">Well-ordered sets</h2>
<h3 id="definition-of-well-ordered-sets">Definition of well-ordered sets</h3>
<p>A <em>total ordering</em>, or just ordering, on a set <span class="math inline">\(X\)</span> is a transitive relation <span class="math inline">\(&lt;\)</span> such that for every <span class="math inline">\(x,y\)</span> in <span class="math inline">\(X\)</span> exactly one of <span class="math inline">\(x &lt; y\)</span>, <span class="math inline">\(x=y\)</span> or <span class="math inline">\(y &lt; x\)</span> is true. A total ordering is called a <em>well-ordering</em> if, in addition, every non-empty subset of <span class="math inline">\(X\)</span> has a minimal element. That is, if <span class="math inline">\(Y\)</span> is a non-empty subset of <span class="math inline">\(X\)</span>, there exists some <span class="math inline">\(y\)</span> in <span class="math inline">\(Y\)</span> such that every <span class="math inline">\(z\)</span> in <span class="math inline">\(Y\)</span> is greater than or equal to <span class="math inline">\(y\)</span>.</p>
<p>The most familiar well-ordering is the usual ordering on the natural numbers: every non-empty subset of <span class="math inline">\(\mathbb{N}\)</span> has a least element. (This fact is discussed a bit more <a href="#induction">below</a>.) A more complicated one was defined earlier, namely the set of all polynomials <span class="math inline">\(p\)</span> with non-negative integer coefficients, with <span class="math inline">\(p &lt; q\)</span> if and only if <span class="math inline">\(p(x) &lt; q(x)\)</span> for all sufficiently large <span class="math inline">\(x\)</span>. To see that this is a well-ordering, let <span class="math inline">\(P\)</span> be a set of such polynomials. From <span class="math inline">\(P\)</span> we can pick all polynomials of least degree. (Note that if <span class="math inline">\(\mathrm{deg}(p) &lt; \mathrm{deg}(q)\)</span> then <span class="math inline">\(p&lt; 1\)</span>.) Amongst those we can select with smallest leading coefficient. (Note that if <span class="math inline">\(p\)</span> has a smaller leading coefficient than <span class="math inline">\(q\)</span> and they are of the same degree, then <span class="math inline">\(p &lt; q\)</span>.) Amongst those we can take the ones with smallest next coefficient, and so on. We end up with a polynomial <span class="math inline">\(p\)</span> which is smaller in the ordering than every other <span class="math inline">\(q\)</span> in <span class="math inline">\(P\)</span>.</p>
<h3 id="maps-between-well-ordered-sets">Maps between well-ordered sets</h3>
<p>A set with a total ordering is called a totally ordered set. A map <span class="math inline">\(f\)</span> between two totally ordered sets <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> is called an <em>order-isomorphism</em> if it is a bijection and <span class="math inline">\(f(x) &lt; f(y)\)</span> if and only if <span class="math inline">\(x &lt; y\)</span>. In other words, it is an invertible map that preserves order. If there is an order-isomorphism between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> then <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are said to be <em>order-isomorphic</em> .</p>
<p>Let <span class="math inline">\(X\)</span> be a well-ordered set. An <em>initial segment</em> of <span class="math inline">\(X\)</span> is a subset of the form <span class="math inline">\(I(z) = \{x:x&lt; z\}\)</span>. A very important fact about well-ordered sets is the following: if <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are well-ordered sets then either <span class="math inline">\(X\)</span> is order-isomorphic to <span class="math inline">\(Y\)</span> or one of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> is order-isomorphic to an initial segment of the other. Here is (a sketch of) how to prove it.</p>
<p>Roughly what we want to do is to take the smallest element of <span class="math inline">\(X\)</span> and pair it with the smallest element of <span class="math inline">\(Y\)</span>, then the next smallest then the next smallest and so on. Of course, this “and so on” is one of our problematic ones, because we may need to pair off the <span class="math inline">\(\omega\)</span>th smallest elements and so on well beyond infinity. However, we can avoid getting into those details by using the well-ordering property.</p>
<p>Let <span class="math inline">\(Z\)</span> be the set of all <span class="math inline">\(z\)</span> in <span class="math inline">\(X\)</span> such that <span class="math inline">\(I(z)\)</span> is order-isomorphic to an initial segment of <span class="math inline">\(Y\)</span>. If <span class="math inline">\(z\)</span> is in <span class="math inline">\(Z\)</span> and <span class="math inline">\(w &lt; z\)</span> then it is easy to see that <span class="math inline">\(w\)</span> is in <span class="math inline">\(Z\)</span>. Hence, either <span class="math inline">\(Z\)</span> is all of <span class="math inline">\(X\)</span> or it is the initial segment <span class="math inline">\(I(u)\)</span>, where <span class="math inline">\(u\)</span> is the least element of <span class="math inline">\(X\)</span> that does not belong to <span class="math inline">\(Z\)</span>.</p>
<p>Next, I claim that if <span class="math inline">\(z\)</span> is in <span class="math inline">\(Z\)</span>, then there is exactly one isomorphism from <span class="math inline">\(I(z)\)</span> to an initial segment of <span class="math inline">\(Y\)</span>. For if not, let <span class="math inline">\(w\)</span> be minimal such that there are two distinct order-isomorphisms <span class="math inline">\(f\)</span> and <span class="math inline">\(g\)</span> from <span class="math inline">\(I(w)\)</span> to initial segments of <span class="math inline">\(Y\)</span>. Let <span class="math inline">\(v\)</span> be minimal such that <span class="math inline">\(f(v)\)</span> does not equal <span class="math inline">\(g(v)\)</span>. Then <span class="math inline">\(f\)</span> and <span class="math inline">\(g\)</span> must agree on the initial segment <span class="math inline">\(I(v)\)</span>, and <span class="math inline">\(f(v)\)</span> and <span class="math inline">\(g(v)\)</span> are then forced to be the least element of <span class="math inline">\(Y\)</span> that does not belong to <span class="math inline">\(f(I(v))\)</span>.</p>
<p>This observation allows us to define an order-isomorphism from <span class="math inline">\(Z\)</span> into <span class="math inline">\(Y\)</span> – each <span class="math inline">\(z\)</span> in <span class="math inline">\(Z\)</span> maps to the least element of <span class="math inline">\(Y\)</span> not included in <span class="math inline">\(f(I(z))\)</span>. Then either <span class="math inline">\(f(Z) = Y\)</span>, in which case we are done, or <span class="math inline">\(f(Z)\)</span> is a proper subset of <span class="math inline">\(Y\)</span>, in which case <span class="math inline">\(Z\)</span> must be the whole of <span class="math inline">\(X\)</span> or we’d be able to extend <span class="math inline">\(f\)</span>.</p>
<h3 id="order-types-of-well-ordered-sets">Order-types of well-ordered sets</h3>
<p>As you may well have noticed, there is an obvious notation for at least some of the elements of a well-ordered set <span class="math inline">\(X\)</span>. We could call the first element <span class="math inline">\(1\)</span>, the next <span class="math inline">\(2\)</span>, the next <span class="math inline">\(3\)</span> and so on. (I can always talk about “the next”: it just means the minimal element not yet mentioned.) If I’ve gone through all the natural numbers and still haven’t included all of <span class="math inline">\(X\)</span>, then I’ll call the next element <span class="math inline">\(\omega\)</span>, the next <span class="math inline">\(\omega+1\)</span> and so on. Then after those I’ll have <span class="math inline">\(2\omega\)</span>, <span class="math inline">\(2\omega+1\)</span> and so on. Then <span class="math inline">\(3\omega\)</span>, <span class="math inline">\(4 \omega\)</span>, <span class="math inline">\(\dots\)</span>, <span class="math inline">\(\omega^2\)</span>, <span class="math inline">\(\dots\)</span>, <span class="math inline">\(\omega^{\omega}\)</span>, <span class="math inline">\(\dots\)</span>, <span class="math inline">\(\omega^{\omega^{\omega}}\)</span>, <span class="math inline">\(\dots\)</span>, <span class="math inline">\(\epsilon_0\)</span>, <span class="math inline">\(\dots\)</span>. I didn’t mention earlier that there are also <span class="math inline">\(\epsilon_1\)</span>, <span class="math inline">\(\epsilon_2\)</span>, <span class="math inline">\(\dots\)</span>, <span class="math inline">\(\epsilon_{\omega}\)</span>, <span class="math inline">\(\epsilon_{\epsilon_0}\)</span>, <span class="math inline">\(\dots\)</span> and even <span class="math inline">\(\epsilon_{\epsilon_{\epsilon_{\dots}}}\)</span>.</p>
<p>The notion of order-isomorphism gives us what we’d like to call an equivalence relation on the set of all well-ordered sets. Unfortunately, we can’t call it that because, for example, every set of the form <span class="math inline">\(\{A\}\)</span>, where <span class="math inline">\(A\)</span> is a set, can be well-ordered, and the object <span class="math inline">\(\{\{A\}: A \; \mbox{is a set}\}\)</span> is too big to be a set. Anyhow, in the back of our minds is the idea that the <em>order type</em> of a well-ordered set is the isomorphism class of that set, but the difficulty just mentioned forces us to define it differently – by picking a representative from each class. That is, we’d like to choose a whole lot of well-ordered sets in such a way that every well-ordered set is order-isomorphic to exactly one of our special ones. We will call them <span class="math inline">\(1\)</span>, <span class="math inline">\(2\)</span>, <span class="math inline">\(3\)</span>, <span class="math inline">\(\dots\)</span>, <span class="math inline">\(\omega\)</span>, <span class="math inline">\(\omega+1\)</span>, <span class="math inline">\(\dots\)</span> etc. and the general name for them will be ordinals.</p>
<h2 id="using">How to use ordinals</h2>
<p>Let us think once again about the three problems we were grappling with earlier. What we were looking for was an <em>indexing set</em> – that is, a set <span class="math inline">\(X\)</span> which we could use to label the terms of an infinite list, or rather something like a list. In each case, although the list would sometimes go on for ever but still not have finished, we could at any stage talk about the <em>next</em> item on the list. This suggests that what we really needed was a well-ordered set. The natural numbers are a very useful well-ordered set and are often used as an indexing set, but for these applications we wanted something larger.</p>
<p>In this section I shall show how the following statement does the job. For the moment we can think of it as an axiom and later I shall prove it.</p>
<p><strong>Axiom:</strong> <em>There exists an uncountable well-ordered set.</em></p>
<p>Let me show how this axiom, together with the two basic lemmas, solves Problem 1 for us, and allows us to conclude that continuous functions on <span class="math inline">\([0, 1]\)</span> are bounded. Recall that we were building a sequence <span class="math inline">\(t_1\)</span>, <span class="math inline">\(t_2\)</span>, <span class="math inline">\(\dots\)</span> which went on well beyond infinity. It is not hard to see that, at least to the point that we took it, this sequence forms a well-ordered subset of <span class="math inline">\(\mathbb{R}\)</span> (under the usual ordering). Indeed, it is order-isomorphic to the set we used to label the sequence – which, though we thought of it as mere notation, does come with a natural ordering.</p>
<p>With that procedure in the back of our minds, let <span class="math inline">\(X\)</span> be an uncountable well-ordered set and let us imagine that we have tried to build a well-ordered subset <span class="math inline">\(T\)</span> of <span class="math inline">\([0, 1]\)</span> indexed by <span class="math inline">\(X\)</span>, so that a typical element of <span class="math inline">\(T\)</span> will be written <span class="math inline">\(t_x\)</span> for some <span class="math inline">\(x\)</span> in <span class="math inline">\(X\)</span>, and the map <span class="math inline">\(x\)</span> goes to <span class="math inline">\(t_x\)</span> will be an order-isomorphism.</p>
<p>Now I can make more formal an argument I gave earlier. For every <span class="math inline">\(x\)</span> in <span class="math inline">\(X\)</span> there is a <em>successor</em>, that is, a least element <span class="math inline">\(y\)</span> of <span class="math inline">\(X\)</span> which is greater than <span class="math inline">\(x\)</span>. Since the map <span class="math inline">\(x\)</span> goes to <span class="math inline">\(t_x\)</span> is an order-isomorphism, there is no element of <span class="math inline">\(T\)</span> between <span class="math inline">\(t_x\)</span> and <span class="math inline">\(t_y\)</span>. Therefore, that interval contains a rational <span class="math inline">\(q_x\)</span> and all those rationals are distinct. That contradicts the fact that <span class="math inline">\(X\)</span>, and hence <span class="math inline">\(T\)</span>, were supposed to be uncountable.</p>
<p>This shows that there is no uncountable well-ordered subset of <span class="math inline">\([0, 1]\)</span>, which means that the “sequence” I constructed must eventually come to an end – by reaching <span class="math inline">\(1\)</span>. To make this formal, let us define the sequence explicitly as follows. First enumerate all the rationals in <span class="math inline">\([0, 1]\)</span>. Now, if I have defined <span class="math inline">\(t_x\)</span> then let <span class="math inline">\(y\)</span> be the successor of <span class="math inline">\(x\)</span>. The first basic lemma tells us that there is an <span class="math inline">\(s&gt;x\)</span> such that <span class="math inline">\(f\)</span> is bounded on the interval <span class="math inline">\([0, s]\)</span>. The interval <span class="math inline">\(\left(x,s\right]\)</span> contains many rational numbers. For the sake of definiteness let <span class="math inline">\(t_y\)</span> be the first rational <span class="math inline">\(q\)</span> in the enumeration such that <span class="math inline">\(f\)</span> is bounded on the interval <span class="math inline">\([0,q]\)</span>. If <span class="math inline">\(y\)</span> is not the successor of any <span class="math inline">\(x\)</span>, then define <span class="math inline">\(t_y\)</span> to be the supremum of all <span class="math inline">\(t_x\)</span> such that <span class="math inline">\(x &lt; y\)</span>. Since <span class="math inline">\(f\)</span> is bounded up to every <span class="math inline">\(t_x\)</span> with <span class="math inline">\(x &lt; y\)</span> and since these get arbitrarily close to <span class="math inline">\(t_y\)</span>, <span class="math inline">\(f\)</span> is bounded up to <span class="math inline">\(t_y\)</span>.</p>
<p>As long as <span class="math inline">\(t_y\)</span> never equals <span class="math inline">\(1\)</span> this defines a unique order isomorphism from <span class="math inline">\(X\)</span> into <span class="math inline">\([0, 1]\)</span>. (If not, let y be the first element of <span class="math inline">\(X\)</span> where anything goes wrong. But I’ve just shown unambiguously how to define <span class="math inline">\(t_y\)</span> once I’ve defined <span class="math inline">\(t_x\)</span> for all previous <span class="math inline">\(x\)</span>.) It follows that for some <span class="math inline">\(y\)</span> <span class="math inline">\(t_y=1\)</span> and this shows that <span class="math inline">\(f\)</span> is bounded on <span class="math inline">\([0, 1]\)</span>.</p>
<hr />
<p>Now let us return to Borel sets. We can measure the complexity of such sets as follows. Say that a set has complexity at most 1 if it is open or closed, complexity at most 2 if it is a countable union of closed sets or a countable intersection of open sets, and in general complexity at most <span class="math inline">\(\mathbb{N}\)</span> if it is a countable union or intersection of sets of complexity at most <span class="math inline">\(n-1\)</span>. If in addition the set is not of complexity at most <span class="math inline">\(n-1\)</span> then say that the set is of complexity <span class="math inline">\(n\)</span>. More generally still, given an element <span class="math inline">\(x\)</span> of <span class="math inline">\(X\)</span>, say that a set has complexity at most <span class="math inline">\(X\)</span> if it is a countable union or intersection of sets each of which has complexity strictly less than x. [As before, this definition makes sense – or there would be a smallest <span class="math inline">\(X\)</span> for which it does not make sense, which there clearly isn't.]</p>
<p>Let us call an element <span class="math inline">\(x\)</span> in <span class="math inline">\(X\)</span> <em>countable</em> if the initial segment <span class="math inline">\(I(x)\)</span> is countable. I now claim that every Borel set has complexity <span class="math inline">\(x\)</span> for some countable <span class="math inline">\(x\)</span>. Let us say that such a set has <em>countable complexity</em>. To prove that, all I need to do is show that a countable union or intersection of sets of countable complexity also has countable complexity. Then it follows that, no matter how much one takes countable unions or intersections, starting with open and closed sets, the result will always have countable complexity.</p>
<p>Let <span class="math inline">\(A_1\)</span>, <span class="math inline">\(A_2\)</span>, <span class="math inline">\(\dots\)</span> be sets with countable complexities <span class="math inline">\(x_1\)</span>, <span class="math inline">\(x_2\)</span>, <span class="math inline">\(\dots\)</span>. I claim first that there is an element <span class="math inline">\(x\)</span> of <span class="math inline">\(X\)</span> that exceeds all of the <span class="math inline">\(x_n\)</span>. To see this, first note that the union of the sets <span class="math inline">\(I(x_n)\)</span> is not the whole of <span class="math inline">\(X\)</span>, since it is a countable union of countable sets and <span class="math inline">\(X\)</span> is uncountable. Now let <span class="math inline">\(x\)</span> be the smallest element of <span class="math inline">\(X\)</span> that belongs to none of the intervals <span class="math inline">\(I(x_n)\)</span>. It is possible that <span class="math inline">\(x=x_n\)</span> for some <span class="math inline">\(n\)</span>, but we can at least say that the successor <span class="math inline">\(y\)</span> of <span class="math inline">\(x\)</span> exceeds every <span class="math inline">\(x_n\)</span>, from which it follows that the union or intersection of the <span class="math inline">\(A_n\)</span> has complexity at most <span class="math inline">\(y\)</span>.</p>
<p>It is not obvious, but can be shown, that for every countable <span class="math inline">\(x\)</span> in <span class="math inline">\(X\)</span> there is a Borel set of complexity <span class="math inline">\(x\)</span>. (Earlier we showed this for <span class="math inline">\(x=2\)</span>, with an unnecessarily complicated example – the rational numbers is another.)</p>
<hr />
<p>Using <span class="math inline">\(X\)</span>, we can also measure the complexity of winning strategies for player <span class="math inline">\(I\)</span> in open games. By <span class="math inline">\(X\)</span> I now mean the uncountable well-ordered set and not the set of all winning sequences for the first player, which I shall call <span class="math inline">\(W\)</span>. All we have to say is that a strategy is of complexity 0 if every sequence is a win for player 1, that is, if <span class="math inline">\(W=\mathbb{N}^{\mathbb{N}}\)</span>, so that player 1 has won before the game even starts. It is of complexity at most <span class="math inline">\(X\)</span> if, whatever player 2 plays for the first move, player 1’s strategy is then of complexity at most <span class="math inline">\(y\)</span> for some <span class="math inline">\(y &lt; x\)</span>. (For example, it is of complexity at most <span class="math inline">\(w\)</span> if after the first pair of moves player 1 can declare, for some <span class="math inline">\(n\)</span>, that the game will definitely be won in the next <span class="math inline">\(n\)</span> moves.) A strategy has complexity <span class="math inline">\(X\)</span> if it has complexity at most <span class="math inline">\(x\)</span> and for no <span class="math inline">\(y &lt; x\)</span> does it have complexity at most <span class="math inline">\(y\)</span>.</p>
<p>Here is a proof that, for every open game, either player 2 has a winning strategy or player 1 has a winning strategy of some countable complexity. Suppose that player 1 does not have such a strategy. Then, whatever player 1 plays, player 2 must have a move such that player 1 still does not have a winning strategy of countable complexity. Why is this? Well, otherwise player 1 could play some <span class="math inline">\(n\)</span> such that, for every possible move nmn of player 2 there was then a winning strategy <span class="math inline">\(f_m\)</span> of complexity some countable <span class="math inline">\(x_m\)</span>. Then the following is a winning strategy for player 1: play <span class="math inline">\(n\)</span> and follow with strategy <span class="math inline">\(f_m\)</span> if player 2 plays <span class="math inline">\(m\)</span>. Since the <span class="math inline">\(x_m\)</span> are countable, there is some <span class="math inline">\(x\)</span> in <span class="math inline">\(X\)</span> that exceeds all of them, so this strategy has complexity at most <span class="math inline">\(x\)</span>.</p>
<p>This proves my claim that player 2 has a move that ensures that player 1 still has no winning strategy of countable complexity. But this argument can now be repeated. In other words, player 2 can play in such a way that player 1 never has a winning strategy of countable complexity, and in particular never wins the game (or else the complexity would be zero). If player 2 plays like this, the result will be some infinite sequence <span class="math inline">\((n_1, n_2, n_3, \dots)\)</span>. If it belonged to <span class="math inline">\(W\)</span>, then there would be some <span class="math inline">\(k\)</span> such that all sequences beginning <span class="math inline">\((n_1, n_2, \dots, n_k)\)</span> were also in <span class="math inline">\(W\)</span> – by the definition of an open game. But this contradicts the fact that player 1 never wins. Hence, player 2 has a way of ensuring that the game results in a sequence not in <span class="math inline">\(W\)</span> – that is, has a winning strategy.</p>
<h2 id="uncountablewoset">Proof that there is an uncountable well-ordered set</h2>
<p>Recall from earlier that we wanted to define ordinals to be order-isomorphism classes of well-ordered sets, but that set-theoretical considerations forbade this. This problem can be avoided if all we need is countable ordinals. Here is a definition that will do: a <em>countable ordinal</em> is an isomorphism class of well-ordered subsets of R. (We proved above that every well-ordered subset of R must be countable.) What, for example, is <span class="math inline">\(\omega+2\)</span>? It is the set of all sets of the form {t<sub>1</sub>,t<sub>2</sub>,...,<span class="math inline">\(u_1\)</span>,<span class="math inline">\(u_2\)</span>}, where the <span class="math inline">\(t_n\)</span> are a bounded increasing sequence, and every <span class="math inline">\(t_n\)</span> is less than <span class="math inline">\(u_1\)</span> which is less than <span class="math inline">\(u_2\)</span>.</p>
<p>How can we now prove that there is an uncountable well-ordered set? All we have to do is use ordinals, as just constructed. Let us see why they come with a natural well-ordering and why there are uncountably many of them.</p>
<p>The ordering is as follows. Write a, b for typical well-ordered subsets of R and [a], [b] for their isomorphism classes. Then [a]&lt;[b] if a is isomorphic to an initial segment of b. Clearly this gives a well-defined total ordering. To see that it is a well-ordering, let A be a set of ordinals and let [a] be an element of A. Then either [a] is minimal or there is a non-empty subset B consisting of all [b] such that b is isomorphic to an initial segment I(t) of a. Pick [b] such that t is minimized (which can be done as a is a well-ordered set of reals). Then [b] must be a minimal element of A.</p>
<p>Now let us prove that there are uncountably many countable ordinals. (There is no paradox here – just as it is not paradoxical that there are infinitely many finite numbers.) If there were countably many, then we could write them as [a<sub>1</sub>],[a<sub>2</sub>],... . Now it is easy to construct an order-preserving bijection from R to the open interval (-1,1) (something built out of tan<sup>-1</sup> will do the job). Hence, given a well-ordered subset a of R and an open interval (c,d), we can find a well-ordered subset of (c,d) that is isomorphic to a.</p>
<p>We can therefore build a set as follows. For each <span class="math inline">\(\mathbb{N}\)</span> let b<sub>n</sub> be an isomorphic copy of a<sub>n</sub> that lives in the interval (n-1,n) and let b the the union of the b<sub>n</sub>. It is easy to see that b is well-ordered – given any non-empty subset first find the minimal <span class="math inline">\(\mathbb{N}\)</span> such that it intersects (n-1,n) and then find the minimal element of that intersection.</p>
<p>Next, I claim that [b] is at least as big as every [a<sub>n</sub>]. If this were not the case, then we would be able to find <span class="math inline">\(\mathbb{N}\)</span> such that b was isomorphic to an initial segment of [a<sub>n</sub>]. But since a<sub>n</sub> embeds into b via the isomorphism with b<sub>n</sub> this would show that [a<sub>n</sub>] was order-isomorphic to a subset of an initial segment I(t) of itself. But it is not hard to show that this is impossible. Let <span class="math inline">\(f\)</span> be the supposed isomorphism and let s be minimal such that f(s)&lt; s. (Such an s exists - t is an example.) But then f(s)=u for some u&lt; s and f(u)<u>&gt;</u>u, so <span class="math inline">\(f\)</span> is not order-preserving.</p>
<p>Finally, if [b] is at least as big as every [a<sub>n</sub>] then the successor of [b] cannot be one of the [a<sub>n</sub>]. (The successor is obtained by taking a bounded isomorphic copy of b, adding in an upper bound and taking the isomorphism class.)</p>
<h2 id="notusing">How not to use ordinals</h2>
<p>Now comes the moment to admit that my `applications' of countable ordinals were, in a sense, a con. The application to Borel sets wasn't really solving a problem – it was just classifying the Borel sets in quite an interesting way. As for the other two results – that continuous functions on <span class="math inline">\([0, 1]\)</span> are bounded and that open games are determined – it is downright silly to use ordinals for their proofs and very easy to remove them. This is almost always true of proofs that use countable ordinals. Though there are probably several counterexamples to this assertion, I myself know of only one theorem proved with countable ordinals for which a neater ordinal-free proof has not been discovered, and even there I am convinced that it exists.</p>
<h3 id="induction">Ordinals and induction</h3>
<p>It is a familiar fact about induction that it can be done in two ways. Given a sequence of statements P(n), either you prove P(0) and that (P(l) for all l&lt; k) ==&gt; P(k) or you say, `Let k be minimal such that P(k) is false' and derive a contradiction. In both cases the actual work is the same – you get the contradiction by showing that P(k) follows from the truth of all previous P(l), unless k=0 in which case you must argue separately.</p>
<p>What we were doing above is very similar to induction except that there are two sorts of induction step. The more straightforward one is showing that a statement P(y) (for some ordinal y in X) implies P(x), where <span class="math inline">\(X\)</span> is the successor of y. The other one is a `limiting' stage, where we show that if <span class="math inline">\(X\)</span> is not the successor of any y, then we can at least deduce the truth of P(x) from the truth of all P(y) for which y&lt; x.</p>
<p>If P(0) is true and both sorts of induction step are proved, then P(x) is true for every <span class="math inline">\(X\)</span> in X. Why? Because if not then there must be some minimal <span class="math inline">\(X\)</span> for which P(x) is false. But then either <span class="math inline">\(X\)</span> is the successor of y for which P(y) is true, or P(y) is true for every y&lt; <span class="math inline">\(X\)</span> (in fact, the second statement is enough on its own) and we have a contradiction.</p>
<p>Although induction and picking a minimal counterexample are formally the same thing, they feel different. With induction one imagines some infinitely long process taking place as one gradually proves more and more of the statement, whereas with minimal counterexamples one has the illusion that the proof takes only one step. For this reason, choosing a minimal counterexample is often cleaner. The same is true for generalized `ordinal induction'.</p>
<h3 id="solving-problems-1-and-3-without-ordinals.">Solving problems 1 and 3 without ordinals.</h3>
<p>Returning to the proof that continuous functions are bounded on <span class="math inline">\([0, 1]\)</span>, here are two ways of removing ordinals. The first is to change slightly how I defined the sequence t<sub>x</sub>. If y is the successor of x, a more natural way to choose t<sub>y</sub> is to make it as large as possible, subject to the constraint that |f(s)-f(t<sub>x</sub>)| should be at most 1 for every s in the interval [t<sub>x</sub>,t<sub>y</sub>]. Luckily, this makes sense – the set of all numbers satisfying the constraint attains its upper bound. Now suppose we generate the sequence t<sub>1</sub>,t<sub>2</sub>,... in this way. Then either it reaches 1 or it converges to some limit t. But there is an interval about t in which <span class="math inline">\(f\)</span> oscillates by at most 1/2, and that interval must contain some t<sub>n</sub>, which proves that t<sub>n+1</sub> is bigger than t – a contradiction.</p>
<p>In other words, it was only because of a perverse definition of t<sub>y</sub> (using an enumeration of the rationals) that there was ever any chance of going beyond w in the ordinal hierarchy.</p>
<p>But once we have noticed this simpler proof, a yet simpler one suggests itself. After all, the number 1 was rather arbitrary above. Why not simply define t<sub>n+1</sub> to be as large as possible subject to the constraint that <span class="math inline">\(f\)</span> should be bounded on the interval [t<sub>n</sub>,t<sub>n+1</sub>]? Again, this makes sense. If <span class="math inline">\(f\)</span> is bounded on the interval <span class="math inline">\([0, s]\)</span> for every s&lt; t, then <span class="math inline">\(f\)</span> is bounded on the interval <span class="math inline">\([0, t]\)</span> (since it is bounded on an interval surrounding t). Setting t<sub>0</sub>=0, what will t<sub>1</sub> be? It can't be less than 1 because then t<sub>1</sub> is surrounded by an interval on which <span class="math inline">\(f\)</span> is bounded, contradicting maximality. So it's 1, and there isn't even a sequence.</p>
<p><span id="induction">This is basically the proof I gave in the page on</span><a href="https://www.dpmms.cam.ac.uk/~wtg10/bounded.html">the boundedness of continuous functions on <span class="math inline">\([0, 1]\)</span></a>, and it can be thought of as looking for a minimal counterexample – focusing on the infimum of the set of all t such that <span class="math inline">\(f\)</span> is not bounded on <span class="math inline">\([0, t]\)</span>, and deriving a contradiction. This is not meant as a formal statement, as I wasn't taking the minimal element of a subset of a well-ordered set, and in fact I do not know of a formal procedure for removing ordinals from proofs, though again I feel that something might be possible (and might be known to set theorists).</p>
<p>I devoted some time to explaining why it wasn't obvious that all games are determined. However, I do not wish to suggest that one's intuition that they are is entirely stupid. Why, for example, is it obvious in chess that either white has a winning strategy or black has a strategy for forcing at least a draw? Well, if white has no winning strategy, then after white's first move there must be some way for black's first move to result in a position for which white still has no winning strategy (or else white could win by seeing what black did and applying the appropriate strategy from that point on). Similarly, after white's second move there must be a move for black that does not give white a winning strategy, and so on. In this way black can avoid defeat, but since the number of moves in a chess game is bounded (if the same position is reached three times then it is a draw), that is all black needs to do to force a draw.</p>
<p>In other words, what one <em>can</em> conclude from player 1 not having a winning strategy is that player 2 has a strategy that makes it possible to avoid losing at any finite stage of the game. If the game is open, this will in fact be a winning strategy. To see this, suppose that the game comes to an end with a sequence <span class="math inline">\((n_1, n_2, n_3, \dots)\)</span> and that at no stage has player 1 won. Then this sequence cannot be a win for player 1, or there would have been some k such that player 1 had won after the k<sup>th</sup> move. Therefore, it is a win for player 2. Note that I reasoned identically at the end of the earlier, ordinals proof. What I have now shown is that the ordinals were a red herring. It is interesting to classify strategies by their ordinal complexity, but quite unnecessary if all one wants is the easy result that open games are determined.</p>
<h2 id="undetermined">A very brief sketch of how to “construct” a game that is not determined</h2>
<p>As I mentioned earlier, this needs the axiom of choice, so this section requires slightly more knowledge than the rest of the page. (I should add that when I say "needs the axiom of choice" I am speaking informally. Some set theorists are fans of the so-called axiom of determinacy, which states that all games are determined. This is definitely not equivalent to the negation of the axiom of choice, even though, as we shall see, it implies it.)</p>
<p>We use the axiom of choice in the following form: every set S can be well-ordered in such a way that all initial segments of the well-ordering have smaller cardinality than that of S. In our case, we consider the set S of all possible strategies for both players of the game, which has the cardinality of the continuum. Imagine that it has been well-ordered in the way specified. Now take the first strategy and suppose that it is a strategy for the first player. Run the game with the first player using this strategy and the second making some arbitrary set of moves. This generates a sequence which we shall place outside <span class="math inline">\(X\)</span> (thus ensuring that the second player's moves result in a sequence not in <span class="math inline">\(X\)</span> and thereby defeat the first strategy.) Now take the second strategy in the well-ordering. Suppose it is a strategy for the second player. Choose arbitrary moves for the first player and run the game with this strategy, <em>except</em> that the first player's moves should be chosen in such a way that the resulting sequence is not the same as the sequence we first produced. This sequence is easy to avoid, though, as there are continuum-many choices for the moves of the first player. Having generated the second sequence, put it in <span class="math inline">\(X\)</span> (ensuring now that the second strategy is not a winning strategy for the second player).</p>
<p>Continue this way, at each stage taking the minimal strategy (in the well-ordering) not yet considered. Since one has at each stage constructed strictly fewer than continuum-many sequences, it is easy to choose moves for the player playing against the strategy under consideration so as to produce an entirely new sequence, which can then be placed in or out of <span class="math inline">\(X\)</span> as is appropriate. When all strategies have been considered, the remaining sequences can be assigned to <span class="math inline">\(X\)</span> or X<sup>c</sup> arbitrarily. And now, in a trivial just-do-it way we have made sure that no strategy is a winning strategy.</p>
<h2 id="infplusone">How can infinity plus one not be infinity?</h2>
<p>There was a time when this question bothered me and made me feel uneasy about ordinals. However, it has a simple and precise answer, one that is quite revealing about the abstract method in mathematics.</p>
<p>Why do people say that infinity plus one equals infinity? Presumably because if <span class="math inline">\(X\)</span> is an infinite set, y is not in <span class="math inline">\(X\)</span> and Y=X union {y}, then Y can be put into one-to-one correspondence with X. Or, even more concretely, the natural numbers can be put into one-to-one correspondence with the non-negative integers via the map f(n)=n-1. So it looks, for example, as though omega+1 ought to be the same size as omega.</p>
<p>Well, what <em>is</em> correct about that last assertion is that a well-ordered set of order type omega can be put into one-to-one correspondence with a well-ordered set of order-type omega+1. However, the one-to-one correspondence will not be order-preserving. In other words, there are two notions of isomorphism that one can choose to be interested in. One is appropriate for sets: two sets are isomorphic if there is a bijection between them. The other is appropriate for well-ordered sets: two well-ordered sets are isomorphic if there is an <em>order-preserving</em> bijection between them. To say that omega and omega+1 are distinct is to say that they are not order-isomorphic. (A small complication: as I defined countable ordinals, they were not well-ordered sets, but equivalence classes of well-ordered sets. To make the above discussion strictly correct, one can change the definition as follows: take the uncountable well-ordered set defined earlier and define a countable ordinal to be any proper initial segment of it. Alternatively, define two ordinals to be order-isomorphic if any two well-ordered sets with the corresponding order types are order-isomorphic.) As a category theorist might put it, the category of sets with arbitrary functions between them is different from the category of well-ordered sets with order-preserving functions between them. Since the second has much more structure, it is not too surprising that it has a wider variety of infinite objects.</p>
<p>One thing this demonstrates is that there is not just one single notion of "infinity" waiting there for its properties to be explored. Rather, we can think of finite numbers in many different ways, and different ways of thinking about them lead to different generalizations to infinite "numbers". Thinking about the sizes of integers leads to cardinals and thinking about their order properties leads to ordinals. In both cases the word "numbers" is slightly misleading.</p>
</body>
</html>
